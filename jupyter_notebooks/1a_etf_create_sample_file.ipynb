{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920380a6",
   "metadata": {},
   "source": [
    "# Notebook 1a: Creates a 'sample' file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c925310",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Create a 'quick & dirty' sample file from earlier step 0 - to see if the API extract processes worked as it should have.\n",
    "- To use for EDA in the next step 1b\n",
    "\n",
    "## Input(s)\n",
    "\n",
    "- The business Variables (below) as gleaned from the business requirements email.\n",
    "- This includes list of ETF tickers, the data date range, and time interval\n",
    "\n",
    "## Output(s)\n",
    "- generates a 'raw_merged.csv' file and stores it into the resources\\raw_merged folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53786915",
   "metadata": {},
   "source": [
    "## Section 1: Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d107ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tishs\\vscode-projects\\etf-close-price\\jupyter_notebooks\n"
     ]
    }
   ],
   "source": [
    "# Usual Dataframe Library\n",
    "import pandas as pd\n",
    "\n",
    "# No System Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check the current working directory\n",
    "import os\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63a062",
   "metadata": {},
   "source": [
    "# Section 2: Define Business Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we find out everything is working, we can use the full list (below) to see if we can run multiple ETF models successfully\n",
    "ticker_list2 = [\n",
    "    'SKY', 'VOO', 'IVV', 'VTI', 'ITOT',    # Core S&P/Total Market\n",
    "    'QQQ', 'QQQE',                         # Nasdaq/Growth\n",
    "    'IWM', 'IWF', 'IWD', 'MDY',            # Small/Mid/Growth/Value\n",
    "    'XLK', 'XLF', 'XLE', 'XLV',             # Tech/Finance/Energy\n",
    "    'TLT', 'BND', 'HYG',                   # Bonds\n",
    "    'VEU', 'EFA'                           # International (US-listed)\n",
    "]\n",
    "\n",
    "#needed for both API data extractions\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "#Only required for Yahoo Finance data extraction\n",
    "interval = \"1d\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a880ef",
   "metadata": {},
   "source": [
    "# Section 3: Define Functions to\n",
    "- Load ALL the ETF files using the above ticker list 2, using a for loop to stack them on top of each other\n",
    "- Load the FRED macroeconomic data\n",
    "- Merge the two dataframes\n",
    "- Save this file, ready to be used in the next step (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_stack_raw_tickerdf(tickers, interval):\n",
    "     \n",
    "    if isinstance(tickers, str): #adds single ticker to a new list (just in case a string is passed instead of a list)\n",
    "        tickers = [tickers]\n",
    "\n",
    "    data = {} #empty dictionary to hold dataframes for all tickers\n",
    "\n",
    "    for ticker in tickers: #gets the list of tickers and goes through them one by one\n",
    "        \n",
    "        #variable to find the correct file path to read in the raw data files\n",
    "        filename_in = f\"..\\\\resources\\\\raw_files\\yfinance\\{ticker}_{interval}_data.csv\"\n",
    "        \n",
    "        #reads in the raw data file for each ticker and adds it to the dictionary\n",
    "        data[ticker] = pd.read_csv(filename_in, \n",
    "                                   parse_dates=['Date'],            #converts the 'Date' column to Pandas datetime format\n",
    "                                   index_col='Date')                #sets the 'Date' column as the index of the dataframe\n",
    "        \n",
    "        #prints the shape of each loaded dataframe - I find this useful to see if the ticker data loaded correctly\n",
    "        print(f\"Loaded {ticker}: {data[ticker].shape}\")\n",
    "\n",
    "    dfs = [] #from a dictionary to an empty list of dataframes for concatenation\n",
    "\n",
    "    for ticker, df in data.items():\n",
    "\n",
    "        df_reset = df.reset_index()                                 #resets index to have 'Date' as a column again\n",
    "        df_reset['Ticker'] = ticker                                 #adds a new column with the ticker name as we will need this to identify which ticker each row belongs to after concatenation\n",
    "        dfs.append(df_reset)                                        #adds the modified dataframe to the list above\n",
    "\n",
    "    #concatenates all dataframes in the list into a single dataframe and returns it    \n",
    "    return pd.concat(dfs, axis=0, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_merge_raw_etf_econ_data(ticker, interval):\n",
    "\n",
    "    raw_stacked_df = load_and_stack_raw_tickerdf(ticker, interval) #uses the function above to load and stack the raw data files into a single dataframe\n",
    "    \n",
    "    #usual print statement to check the shape of the stacked dataframe for audit purposes\n",
    "    print(f\"\\nFinal stacked raw dataframe shape: {raw_stacked_df.shape}\") \n",
    "    \n",
    "    macro_df = pd.read_csv(f\"..\\\\resources\\\\raw_files\\\\fred\\\\fred_macro_data.csv\") #loads in the single macroeconomic data file created in notebook 0\n",
    "\n",
    "    #for audit purposes - prints the shape of the macroeconomic data file \n",
    "    print('\\nMacro file shape:', \n",
    "          macro_df.shape)\n",
    "\n",
    "    #need to 'align' the date formats in both dataframes before merging them together\n",
    "    raw_stacked_df[\"Date\"] = pd.to_datetime(raw_stacked_df[\"Date\"], \n",
    "                                            format='mixed',            # so as to handle any varied & inconsistent date formats\n",
    "                                            dayfirst=True)             # cuts out the guessing factor, by asking pandas to see ambiguous dates as day first (US vs UK date format styles)\n",
    "    \n",
    "    macro_df[\"Date\"] = pd.to_datetime(macro_df[\"Date\"], \n",
    "                                      format='mixed',                   # see abpve\n",
    "                                      dayfirst=True)                    # see above\n",
    "\n",
    "    merged1 = raw_stacked_df.merge(macro_df, \n",
    "                                   on=\"Date\",                           # merges both dataframes on the Date column - as required by the business case\n",
    "                                   how=\"inner\").fillna(0)               # only select and keep dates from both dataframes - then fill any blanks after the joining with 0\n",
    "\n",
    "    #Some admin is needed here - Moving the Ticker column to the second position - at the front of the file after Date\n",
    "    col = merged1.pop(\"Ticker\")\n",
    "    merged1.insert(1, \n",
    "                   \"Ticker\", \n",
    "                   col)\n",
    "\n",
    "    print('\\nMerged file shape:',                                       #seeing what the shape of the new file looks like\n",
    "          merged1.shape)\n",
    "\n",
    "    merged1.to_csv(f\"..\\\\resources\\\\raw_merged\\\\raw_merged.csv\",        #finally creating a new file and storing it in the raw_merged folder\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993d025",
   "metadata": {},
   "source": [
    "# Section 4 - Running the functions and creating the file for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1ae3162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SKY: (1507, 7)\n",
      "Loaded VOO: (1507, 8)\n",
      "Loaded IVV: (1507, 8)\n",
      "Loaded VTI: (1507, 8)\n",
      "Loaded ITOT: (1507, 8)\n",
      "Loaded QQQ: (1507, 8)\n",
      "Loaded QQQE: (1507, 8)\n",
      "Loaded IWM: (1507, 8)\n",
      "Loaded IWF: (1507, 8)\n",
      "Loaded IWD: (1507, 8)\n",
      "Loaded MDY: (1507, 8)\n",
      "Loaded XLK: (1507, 8)\n",
      "Loaded XLF: (1507, 8)\n",
      "Loaded XLE: (1507, 8)\n",
      "Loaded XLV: (1507, 8)\n",
      "Loaded TLT: (1507, 8)\n",
      "Loaded BND: (1507, 8)\n",
      "Loaded HYG: (1507, 8)\n",
      "Loaded VEU: (1507, 8)\n",
      "Loaded EFA: (1507, 8)\n",
      "\n",
      "Final stacked raw dataframe shape: (30140, 10)\n",
      "\n",
      "Macro file shape: (2190, 47)\n",
      "\n",
      "Merged file shape: (30120, 56)\n"
     ]
    }
   ],
   "source": [
    "stack_and_merge_raw_etf_econ_data(ticker_list2, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4f007",
   "metadata": {},
   "source": [
    "### Note for the Bug List\n",
    "\n",
    "- Not an actual bug as such, but those with a keen eye will noticed the disparity between the stacked ETF raw dataframe shape and merged file shapes above\n",
    "- I've recorded this on an Excel file in the same directory (for sake of audit/completeness again)\n",
    "- This was because the ETF file had data for a date that the FRED Macro file did not - 20 records were therefore dropped for the date - 02/01/2020\n",
    "- I found a similar date disparity issue later on - as sometimes a working day in one file is a holiday date in another!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ebcd2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
